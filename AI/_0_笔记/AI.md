## 数据集配置

> ### 配置方式
>
> + 训练集、验证集、测试集
>   + 先用训练集训练出一组参数，然后用验证集来评估这组参数的表现，如果表现不好就调整超参数（学习率、神经网络层数等），再用训练集训练出一组参数，然后再验证，知道验证结果满意了，再用最终的测试集进行参数评估，如果最终测试集的评估结果不好的话，就需要重新训练，从这个角度讲，测试集就变成了验证集了。
> + 训练集、测试集
>   + 所以很多人就直接划分成这两个数据集

>### 划分比例
>
>+ 训练集、验证集、测试集：
>  + 小型数据集：6：2：2
>  + 大型数据集：98：1：1 或 99：5：0.4：0.1
>+ 训练集、测试集
>  + 小型数据集：7：3
>  + 大型数据集：。。。

> ### 注意事项
>
> + 要尽量保证训练集、验证集、测试集的数据来源是以致的

## 欠拟合和过拟合

> ### 欠拟合【高偏差】
>
> + 模型与训练数据的匹配程度不好
>   + 模型对**训练数据集**的预测很不准确
> + 

> ### 过拟合【高方差】
>
> + 模型与训练数据的匹配程度过头了
>   + 模型对**训练数据集**的预测准确度很高，但是对**验证数据集**的预测准确率却很低

> ### 解决方法
>
> + 欠拟合
>
>   + 尝试更大的神经网络【**肯定有帮助**，增加神经网络层数、增加神经元个数】
>   + 增加训练次数【**可能**会解决欠拟合】
>   + 尝试其他优化算法【**可能**】
>   + 尝试不同的神经网络架构【**可能**】
>
> + 过拟合
>
>   + 获取更多的训练数据【最佳】【贵】
>
>   + 使用正则化【**可能**】【首选】
>
>     + L2【常见】【权重衰减】【λ：超参数】
>
>       > 在**成本函数**后面加正则化
>       >
>       > ![image-20200512165735291](.\image\L2_1.png)
>       >
>       > ![image-20200512165804646](.\image\L2_2.png)
>       >
>       > ![image-20200512170012509](.\image\L2_3.png)
>       >
>       > 在**计算偏导数**的时候加上正则化
>       >
>       > ![image-20200512165931973](.\image\L2_dw.png)
>
>     + dropout【流行】【图像识别领域】【inverted dropout：反向随机失活】
>
>       > ##### 注意事项：
>       >
>       > 开始训练前关闭dropout，检查代码是否有误，无误时再开启dropout进行训练
>
>     + L1【很少使用】
>
>       > ![image-20200512165902525](.\image\L1.png)
>
>   + 尝试不同的神经网络架构【**可能**】
>
> + 同时解决的方法
>
>   + 在使用了合适的正则化之后，使用更大的神经网络，获取更多的训练数据就能同时解决欠拟合和过拟合问题

> ### 注意事项：
>
> + 判断过拟合与欠拟合的前提条件
>
>   + 训练集与验证集的来源是一致的【如果来源不一致，他们不能相比较】
>
>   + 数据集中的数据时人类容易识别出来的【人类识别的误差被称之为贝叶斯误差】
>
>     

## 数据增强

## 归一化

>### 归一化 

> ### 反归一化 

> ### 归一化隐藏层

## 梯度消失 / 梯度爆炸

> ### 梯度消失
>
> + 导致偏导数极端的小，以至于很多层学习的很慢几乎都不学习了，很多层就变成了了僵尸层，深度神经网络就变成了了浅层神经网络

> ### 梯度爆炸
>
> + 偏导数极端的大，导致很多层乱学习，走火入魔，而且数值可能大到超出了合法范围的值，变成了NaN值，从而导致无法再训练

> ### 梯度检验

> ### 注意事项
>
> + 只在需要时才开启梯度检验【计算量大，一直开着浪费资源】
> + 检验出梯度偏差后，
> + 进一步定位出是哪些参数相应的梯度有问题
> + 如果加了正则化尾巴，那么在梯度检验时也需要带着尾巴
> + 梯度检验和dropout不能同时使用
> + 可以进行多次检验

## 优化

> ### mini-batch
>
> + batch梯度下降：用整个训练集进行梯度下降【学习周期长】【硬件受不了】
> + mini-batch梯度下降：用子训练集做梯度下降
> + 随机梯度下降：将一个样本当做一个子训练集进行梯度下降
> + epoch：对整个训练集进行了一次梯度下降
> + iteration：进行了一次梯度下降

> ### 指数加权平均【是动量梯度下降及其他很多算法的基础】
>
> + ##### 公式
>
>   + V0 = 0
>   + V1 = (1-k)  \* W1 + k  \*  V0
>   + V2 = (1-k)  \* W2 + k  \*  V1
>
> + ##### 修正第一个值
>
>   + 比如第一个值为W1，那么V1 = （1-k）\* W1
>   + 使用Vt = Vt / (1-k^t)
>   + 一般不适用，因为只是前面一点点偏离，无伤大雅【适用Adam算法时会修正】

> ### momentum动量梯度下降【优于标准梯度下降算法】【适用各种网络结构】

> ### RMSprop全方根prop算法

> ### Adam算法【自适应矩估计算法】
>
> + 将momentum算法和RMSprop算法结合在一起
> + 三个超参数
>   + 动量指数平均值中的超参数K1【通常0.9】
>   + RMSprop中的超参数K2【通常0.999】
>   + 学习率r【主要调整这个】

> ### 学习率衰减【超参数】
>
> + 控制学习率衰减的速度
>
> + 公式
>   + R1 = (1 / (decayRate * epochNum)) * R0
>   + R1 = 0.95^epochNum * R0
>   + R1 = k / square(epochNum) * R0 【k：常量】
>   + R1 = k / square(t) * R0【t：梯度下降的次数】
> + 手动衰减
> + 局部最优
>   + 局部最优：完全没有路径向下通往全局最优的小坑
>   + 鞍点：有路径可以通向全局最优的小坑
>   + 多维度的图像中，那些小坑是不会阻碍我们找到全局最优解，因为那些小坑中依然有某些维度可以通向全局最优
>   + 局部最优已经是不需要担心的问题了，但是鞍点依然给我们带来了麻烦。因为无论是局部最优还是鞍点，他们都是一个小坑，而小坑底部的梯度是为0的，没有斜率，而靠近0靠近底部的那片区域斜率也都是很小的，这片区域称之为平稳段，斜率小意味着学习的会很慢。我们可能会花很长时间来到达鞍点的最小值处，然后再从这个点继续往下找全局最优点。
>   + 鞍点和平稳段无法避免，我们只能用我们学习的那些优化算法让神经网络学习得更快，更快的度过那些平稳段

## 超参数

> + 学习率α 【1】
> + 每层神经网络神经元个数n 【2】
> + 子训练集mini-batch的大小【一般为2的次方】 【2】
> + 动量【梯度下降】指数加权平均值的β变量【一般设置为0.9】 【2】
> + 神经网络层数L 【3】
> + 学习率衰减控制参数decayRate 【3】
> + RMSprop中的超参数K2变量【一般设置为0.999】
> + Adam中的K1、K2以及u（防止除0）

## 调试

> ### 调参
>
> + 网格搜索法【效率低】【不使用】
> + 随机搜索法【随机均匀采样】【一般使用】
>   + 线性标尺
>   + 指数标尺【对值的变化很敏感的情况下使用，比如加权平均数】
>     + 示例：r = -4 * np.random.rand() 生成[-4,0]之间的随机数，使用α = 10^r生成学习率
>
> ### 调参模式
>
> + 熊猫宝宝
>   + 同一时刻只训练一个或几个模型来进行实时调参
>   + 有海量训练数据
>   + 硬件资源不足
>   + 需要不断的观察模型的状况，不断地调整和尝试不同的超参数的值。最初随机初始化一些超参数值，然后观察模型的表现状况【损失曲线、验证集的错误率曲线】，如果表现不错，可以尝试调大学习率，继续观察，如果仍然表现不错，可以尝试加入动量梯度下降算法继续观察，如果表现的糟糕起来了，可以回退到之前的超参数值

## softmax

> ### 特点
>
> + 多分类【激活函数】

> ### 步骤
>
> + 4分类，那么Z.shape = (4,1)
> + t = e^Z
> + a = t / np.sum(t)

> ###  损失函数
>
> + ![image-20200512164816194](.\image\Loss_softmax.png)
>
> ### 成本函数
>
> + ![image-20200512165021983](E:\workspace\python\AI\_0_笔记\image\cost_softmax.png)

## 选择框架

> + 选择最方便的。最方便编程、最方便维护、最方便发布
> + 选择运算速度快的，因为神经网络领域对速度是很有要求的。因为我们的数据量特别大，选择一款速度最快的框架可能会节约很多时间、精力、金钱
> + 选择真正开源的。开源可以使框架成长的越来越快。
> + 考虑擅长的语言
> + 选择适合研究领域的框架。因为某些框架可能对视觉领域比较优秀，有些框架对语音领域比较优秀