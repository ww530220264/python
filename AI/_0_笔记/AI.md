## 数据集配置

> ### 配置方式
>
> + 训练集、验证集、测试集
>   + 先用训练集训练出一组参数，然后用验证集来评估这组参数的表现，如果表现不好就调整超参数（学习率、神经网络层数等），再用训练集训练出一组参数，然后再验证，知道验证结果满意了，再用最终的测试集进行参数评估，如果最终测试集的评估结果不好的话，就需要重新训练，从这个角度讲，测试集就变成了验证集了。
> + 训练集、测试集
>   + 所以很多人就直接划分成这两个数据集

> ### 数据集区别
>
> + 验证集：或者用来判断哪一个网络更好
>   + 有些开发人员把验证集叫做【开发集】
>   + 不用测试集时，也有人把验证集叫做测试集
> + 测试集：用来判断出训练好的网络在发布出去后的实际性能有多高
> + 注意事项：有了验证集为什么还需要测试集？
>   + 因为我们是基于验证集来不断优化迭代神经网络的，那么就有可能导致训练出来的神经网络只在验证集上面表现的很好，发布出去后，在市场上表现的很差。所以说，对于正式的项目，建议使用验证集和测试集

>### 划分比例
>
>+ 训练集、验证集、测试集：
>  + 小型数据集：6：2：2
>  + 大型数据集：98：1：1 或 99：5：0.4：0.1
>+ 训练集、测试集
>  + 小型数据集：7：3
>  + 大型数据集：。。。

> ### 注意事项
>
> + 要尽量保证训练集、验证集、测试集的数据来源是一致的
> + 验证集与测试集的来源要一致

## 欠拟合和过拟合

> ### 欠拟合【高偏差】
>
> + 模型与训练数据的匹配程度不好
>   + 模型对**训练数据集**的预测很不准确
> + 

> ### 过拟合【高方差】
>
> + 模型与训练数据的匹配程度过头了
>   + 模型对**训练数据集**的预测准确度很高，但是对**验证数据集**的预测准确率却很低

> ### 解决方法
>
> + 欠拟合
>
>   + 尝试更大的神经网络【**肯定有帮助**，增加神经网络层数、增加神经元个数】
>   + 增加训练次数【**可能**会解决欠拟合】
>   + 尝试其他优化算法【**可能**】
>   + 尝试不同的神经网络架构【**可能**】
>
> + 过拟合
>
>   + 获取更多的训练数据【最佳】【贵】
>
>   + 使用正则化【**可能**】【首选】
>
>     + L2【常见】【权重衰减】【λ：超参数】
>
>       > 在**成本函数**后面加正则化
>       >
>       > ![image-20200512165735291](.\image\L2_1.png)
>       >
>       > ![image-20200512165804646](.\image\L2_2.png)
>       >
>       > ![image-20200512170012509](.\image\L2_3.png)
>       >
>       > 在**计算偏导数**的时候加上正则化
>       >
>       > ![image-20200512165931973](.\image\L2_dw.png)
>
>     + dropout【流行】【图像识别领域】【inverted dropout：反向随机失活】
>
>       > ##### 注意事项：
>       >
>       > 开始训练前关闭dropout，检查代码是否有误，无误时再开启dropout进行训练
>
>     + L1【很少使用】
>
>       > ![image-20200512165902525](.\image\L1.png)
>
>   + 尝试不同的神经网络架构【**可能**】
>
> + 同时解决的方法
>
>   + 在使用了合适的正则化之后，使用更大的神经网络，获取更多的训练数据就能同时解决欠拟合和过拟合问题

> ### 注意事项：
>
> + 判断过拟合与欠拟合的前提条件
>
>   + 训练集与验证集的来源是一致的【如果来源不一致，他们不能相比较】
>
>   + 数据集中的数据时人类容易识别出来的【人类识别的误差被称之为贝叶斯误差】
>
>     

## 数据增强

## 归一化

>### 归一化 

> ### 反归一化 

> ### 归一化隐藏层

## 梯度消失 / 梯度爆炸

> ### 梯度消失
>
> + 导致偏导数极端的小，以至于很多层学习的很慢几乎都不学习了，很多层就变成了了僵尸层，深度神经网络就变成了了浅层神经网络

> ### 梯度爆炸
>
> + 偏导数极端的大，导致很多层乱学习，走火入魔，而且数值可能大到超出了合法范围的值，变成了NaN值，从而导致无法再训练

> ### 梯度检验

> ### 注意事项
>
> + 只在需要时才开启梯度检验【计算量大，一直开着浪费资源】
> + 检验出梯度偏差后，
> + 进一步定位出是哪些参数相应的梯度有问题
> + 如果加了正则化尾巴，那么在梯度检验时也需要带着尾巴
> + 梯度检验和dropout不能同时使用
> + 可以进行多次检验

## 优化

> ### mini-batch
>
> + batch梯度下降：用整个训练集进行梯度下降【学习周期长】【硬件受不了】
> + mini-batch梯度下降：用子训练集做梯度下降
> + 随机梯度下降：将一个样本当做一个子训练集进行梯度下降
> + epoch：对整个训练集进行了一次梯度下降
> + iteration：进行了一次梯度下降

> ### 指数加权平均【是动量梯度下降及其他很多算法的基础】
>
> + ##### 公式
>
>   + V0 = 0
>   + V1 = (1-k)  \* W1 + k  \*  V0
>   + V2 = (1-k)  \* W2 + k  \*  V1
>
> + ##### 修正第一个值
>
>   + 比如第一个值为W1，那么V1 = （1-k）\* W1
>   + 使用Vt = Vt / (1-k^t)
>   + 一般不适用，因为只是前面一点点偏离，无伤大雅【适用Adam算法时会修正】

> ### momentum动量梯度下降【优于标准梯度下降算法】【适用各种网络结构】

> ### RMSprop全方根prop算法

> ### Adam算法【自适应矩估计算法】
>
> + 将momentum算法和RMSprop算法结合在一起
> + 三个超参数
>   + 动量指数平均值中的超参数K1【通常0.9】
>   + RMSprop中的超参数K2【通常0.999】
>   + 学习率r【主要调整这个】

> ### 学习率衰减【超参数】
>
> + 控制学习率衰减的速度
>
> + 公式
>   + R1 = (1 / (decayRate * epochNum)) * R0
>   + R1 = 0.95^epochNum * R0
>   + R1 = k / square(epochNum) * R0 【k：常量】
>   + R1 = k / square(t) * R0【t：梯度下降的次数】
> + 手动衰减
> + 局部最优
>   + 局部最优：完全没有路径向下通往全局最优的小坑
>   + 鞍点：有路径可以通向全局最优的小坑
>   + 多维度的图像中，那些小坑是不会阻碍我们找到全局最优解，因为那些小坑中依然有某些维度可以通向全局最优
>   + 局部最优已经是不需要担心的问题了，但是鞍点依然给我们带来了麻烦。因为无论是局部最优还是鞍点，他们都是一个小坑，而小坑底部的梯度是为0的，没有斜率，而靠近0靠近底部的那片区域斜率也都是很小的，这片区域称之为平稳段，斜率小意味着学习的会很慢。我们可能会花很长时间来到达鞍点的最小值处，然后再从这个点继续往下找全局最优点。
>   + 鞍点和平稳段无法避免，我们只能用我们学习的那些优化算法让神经网络学习得更快，更快的度过那些平稳段

## 超参数

> + 学习率α 【1】
> + 每层神经网络神经元个数n 【2】
> + 子训练集mini-batch的大小【一般为2的次方】 【2】
> + 动量【梯度下降】指数加权平均值的β变量【一般设置为0.9】 【2】
> + 神经网络层数L 【3】
> + 学习率衰减控制参数decayRate 【3】
> + RMSprop中的超参数K2变量【一般设置为0.999】
> + Adam中的K1、K2以及u（防止除0）

## 调试

> ### 调参
>
> + 网格搜索法【效率低】【不使用】
> + 随机搜索法【随机均匀采样】【一般使用】
>   + 线性标尺
>   + 指数标尺【对值的变化很敏感的情况下使用，比如加权平均数】
>     + 示例：r = -4 * np.random.rand() 生成[-4,0]之间的随机数，使用α = 10^r生成学习率
>
> ### 调参模式
>
> + 熊猫宝宝
>   + 同一时刻只训练一个或几个模型来进行实时调参
>   + 有海量训练数据
>   + 硬件资源不足
>   + 需要不断的观察模型的状况，不断地调整和尝试不同的超参数的值。最初随机初始化一些超参数值，然后观察模型的表现状况【损失曲线、验证集的错误率曲线】，如果表现不错，可以尝试调大学习率，继续观察，如果仍然表现不错，可以尝试加入动量梯度下降算法继续观察，如果表现的糟糕起来了，可以回退到之前的超参数值

## softmax

> ### 特点
>
> + 多分类【激活函数】

> ### 步骤
>
> + 4分类，那么Z.shape = (4,1)
> + t = e^Z
> + a = t / np.sum(t)

> ###  损失函数
>
> + ![image-20200512164816194](.\image\Loss_softmax.png)
>
> ### 成本函数
>
> + ![image-20200512165021983](.\image\cost_softmax.png)

## 选择框架

> + 选择最方便的。最方便编程、最方便维护、最方便发布
> + 选择运算速度快的，因为神经网络领域对速度是很有要求的。因为我们的数据量特别大，选择一款速度最快的框架可能会节约很多时间、精力、金钱
> + 选择真正开源的。开源可以使框架成长的越来越快。
> + 考虑擅长的语言
> + 选择适合研究领域的框架。因为某些框架可能对视觉领域比较优秀，有些框架对语音领域比较优秀

## 正交化

## 判断哪个网络更好

> ### 单一数值指标
>
> + 查准率 【Precision】
> + 查全率 【recall】
> + F1分数 【2/(1/P+1/R)】

> ### 优化指标和满足指标
>
> > 有时无法将多个指标融合成一个单一的数值指标，对于这种情况，我们的解决方案是设计一个【优化指标】和N个【满足指标】
>
> > 一般我们只会设计一个优化指标，但是会有多个满足指标。我们会给这些满足指标设计指定一个阈值，不满足这些阈值的网络统统淘汰掉。然后在剩余的网络中选取一个优化指标最高的。

## 贝叶斯误差

### 

> ### 概述
>
> + 一般来说，在一个人工智能项目中，当程序的能力还没有达到人类的能力之前，项目进展缓慢，程序能力提升速度是非常快的，一旦程序能力超过了人类能力【人类误差】之后，提升速度就非常缓慢了。程序能力会非常缓慢的向着理论上的最高能力接近。无论是增加训练时间还和训练数据或者增大训练网络，都无法超越这个理论上的最高能力。【这个理论的上限我们称之为贝叶斯最优误差】
>
> + 原因
>   + 人类能力与最优能力已经非常接近了，所以一旦程序能力超过了人类能力，那么他可以提升的空间已经很小了
>   + 一旦程序超过了人类的能力，那么很多必须依靠人的提升手段，就无法再使用了
> + 超越人类能力之前提升程序能力的手段
>   + 雇佣更多的人来为数据打标签，那么就有更多的数据来供程序进行学习，一旦超过了人类的能力，人都已经无法识别了，也就无法再打标签了。
>   + 只要程序还比不上人类，那么我们就可以人为的分析它为什么比不上人类
>   + 在未超过人类的能力之前，我们可以更好的分析过拟合和欠拟合的情况。
> + 总结
>   + 因此，那些模仿人类的项目进展的很快，就是因为我们可以借助上面这些手段

> ### 判断拟合度
>
> + 可以用人类误差来判断神经网络是否过拟合了【因为人类误差和贝叶斯误差很接近】
> + 神经网络的误差与贝叶斯误差的差距也被叫做可避免误差，比如，神经网络的误差为7%和贝叶斯误差为1%，那么可避免的误差为6%。也就是说，这个神经网络的可提升空间只有6%,因为贝叶斯误差是最优误差，不可能被超越。如果超越了，那么这个神经网络肯定是过拟合了

